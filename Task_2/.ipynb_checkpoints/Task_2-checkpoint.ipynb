{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UGCA Group Assignment -2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Members\n",
    "1. Chetna Singhal (cs57926)\n",
    "2. David Kinman\n",
    "3. Prajval Gupta\n",
    "4. Subhayu Chakravarty\n",
    "5. Whitt Hyde"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART A - Collecting tweets using Tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tweepy as tw\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from string import punctuation\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('secrets.txt', 'r') as file:\n",
    "    data = file.read().split('\\n')\n",
    "    \n",
    "consumer_key = data[0]\n",
    "consumer_secret = data[1]\n",
    "access_key = data[2]\n",
    "access_secret = data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_key, access_secret)\n",
    "api = tw.API(auth,wait_on_rate_limit=True,\n",
    "    wait_on_rate_limit_notify=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_tweets(query, num=0):\n",
    "    num = 3000 if num > 3000 else num\n",
    "    max_num_per_call = 100\n",
    "\n",
    "    #authorize twitter, initialize tweepy\n",
    "    auth = tw.OAuthHandler(consumer_key, consumer_secret)\n",
    "    auth.set_access_token(access_key, access_secret)\n",
    "    api = tw.API(auth)\n",
    "    \n",
    "    #initialize a list to hold all the tweepy Tweets\n",
    "    alltweets = []    \n",
    "    \n",
    "    #make initial request for most recent tweets (200 is the maximum allowed count)\n",
    "    curr_count = max_num_per_call if num > max_num_per_call else num\n",
    "    num -= curr_count\n",
    "\n",
    "    new_tweets = api.search(q=query, count=curr_count)\n",
    "    \n",
    "    #save most recent tweets\n",
    "    alltweets.extend(new_tweets)\n",
    "    \n",
    "    #save the id of the oldest tweet less one\n",
    "    oldest = alltweets[-1].id - 1\n",
    "    \n",
    "    print(f\"{len(alltweets)} tweets downloaded so far\")\n",
    "\n",
    "    #keep grabbing tweets until there are no tweets left to grab\n",
    "    while num > 0:\n",
    "        print(f\"Getting tweets before {oldest}\")\n",
    "        \n",
    "        curr_count = max_num_per_call if num > max_num_per_call else num\n",
    "\n",
    "        #all subsiquent requests use the max_id param to prevent duplicates\n",
    "        new_tweets = api.search(q=query, count=curr_count, max_id=oldest)\n",
    "        num -= curr_count\n",
    "        \n",
    "        #save most recent tweets\n",
    "        alltweets.extend(new_tweets)\n",
    "        \n",
    "        #update the id of the oldest tweet less one\n",
    "        oldest = alltweets[-1].id - 1\n",
    "        \n",
    "        print(f\"{len(alltweets)} tweets downloaded so far\")\n",
    "    \n",
    "    #transform the tweepy tweets into a 2D array that will populate the csv    \n",
    "    outtweets = [[tweet.id_str, tweet.created_at, tweet.text.encode(\"utf-8\"), tweet.user.location] for tweet in alltweets]\n",
    "    df = pd.DataFrame(outtweets, columns=[\"id\", \"created_at\", \"text\", \"location\"])\n",
    "    #df.to_csv(f\"query_{query}.csv\", index=False)\n",
    "    #print(df.head())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets downloaded so far\n",
      "Getting tweets before 1184329809771450367\n",
      "200 tweets downloaded so far\n",
      "Getting tweets before 1184307877546209279\n",
      "300 tweets downloaded so far\n",
      "Getting tweets before 1184299498266189823\n",
      "395 tweets downloaded so far\n",
      "Getting tweets before 1184284840838995969\n",
      "495 tweets downloaded so far\n",
      "Getting tweets before 1184274673120563200\n",
      "595 tweets downloaded so far\n",
      "Getting tweets before 1184261928501137407\n",
      "695 tweets downloaded so far\n",
      "Getting tweets before 1184238665255587839\n",
      "795 tweets downloaded so far\n",
      "Getting tweets before 1184208955322617855\n",
      "895 tweets downloaded so far\n",
      "Getting tweets before 1184164978439458815\n",
      "995 tweets downloaded so far\n",
      "Getting tweets before 1184132878713663487\n",
      "1095 tweets downloaded so far\n",
      "Getting tweets before 1184101888637177856\n",
      "1195 tweets downloaded so far\n",
      "Getting tweets before 1183994484306169855\n",
      "1291 tweets downloaded so far\n",
      "Getting tweets before 1183910614076399615\n",
      "1391 tweets downloaded so far\n",
      "Getting tweets before 1183846886274490367\n",
      "1491 tweets downloaded so far\n",
      "Getting tweets before 1183807512673423360\n",
      "1591 tweets downloaded so far\n",
      "Getting tweets before 1183769308628955137\n",
      "1691 tweets downloaded so far\n",
      "Getting tweets before 1183725111767109631\n",
      "1791 tweets downloaded so far\n",
      "Getting tweets before 1183584852244750335\n",
      "1891 tweets downloaded so far\n",
      "Getting tweets before 1183481073461878784\n",
      "1991 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "# pass in the search query\n",
    "new_search = \"#2020Election\" + \" -filter:retweets\"\n",
    "tweets1 = get_query_tweets(new_search, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets downloaded so far\n",
      "Getting tweets before 1184135398638809090\n",
      "200 tweets downloaded so far\n",
      "Getting tweets before 1184113669296115713\n",
      "300 tweets downloaded so far\n",
      "Getting tweets before 1183933721667223551\n",
      "400 tweets downloaded so far\n",
      "Getting tweets before 1183078457112182784\n",
      "500 tweets downloaded so far\n",
      "Getting tweets before 1182665217840500738\n",
      "600 tweets downloaded so far\n",
      "Getting tweets before 1182191926868660233\n",
      "700 tweets downloaded so far\n",
      "Getting tweets before 1181970493005586431\n",
      "800 tweets downloaded so far\n",
      "Getting tweets before 1181933215185084416\n",
      "900 tweets downloaded so far\n",
      "Getting tweets before 1181701793287430144\n",
      "1000 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "# pass in the search query\n",
    "tweets2 = get_query_tweets(\"2020 U.S. election\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets downloaded so far\n",
      "Getting tweets before 1184405018918690815\n",
      "200 tweets downloaded so far\n",
      "Getting tweets before 1184372849957470207\n",
      "300 tweets downloaded so far\n",
      "Getting tweets before 1184348545706536959\n",
      "400 tweets downloaded so far\n",
      "Getting tweets before 1184327818861252607\n",
      "500 tweets downloaded so far\n",
      "Getting tweets before 1184314003465654271\n",
      "600 tweets downloaded so far\n",
      "Getting tweets before 1184302087280218112\n",
      "700 tweets downloaded so far\n",
      "Getting tweets before 1184287804932538367\n",
      "800 tweets downloaded so far\n",
      "Getting tweets before 1184280923174367231\n",
      "900 tweets downloaded so far\n",
      "Getting tweets before 1184265112636612607\n",
      "1000 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "# pass in the search query\n",
    "tweets3 = get_query_tweets(\"2020 presidential election\", 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 tweets downloaded so far\n",
      "Getting tweets before 1184338908399583231\n",
      "200 tweets downloaded so far\n",
      "Getting tweets before 1184308601348935679\n",
      "300 tweets downloaded so far\n",
      "Getting tweets before 1184298319805370368\n",
      "400 tweets downloaded so far\n",
      "Getting tweets before 1184285351990435839\n",
      "500 tweets downloaded so far\n",
      "Getting tweets before 1184275071470166016\n",
      "600 tweets downloaded so far\n",
      "Getting tweets before 1184268397032292351\n",
      "700 tweets downloaded so far\n",
      "Getting tweets before 1184261835056406527\n",
      "800 tweets downloaded so far\n",
      "Getting tweets before 1184248752384741375\n",
      "900 tweets downloaded so far\n",
      "Getting tweets before 1184211369060687871\n",
      "1000 tweets downloaded so far\n",
      "Getting tweets before 1184184453200875519\n",
      "1100 tweets downloaded so far\n",
      "Getting tweets before 1184155366986268671\n",
      "1200 tweets downloaded so far\n",
      "Getting tweets before 1184126499458301952\n",
      "1300 tweets downloaded so far\n",
      "Getting tweets before 1184096229061550080\n",
      "1400 tweets downloaded so far\n",
      "Getting tweets before 1184049685230698495\n",
      "1500 tweets downloaded so far\n",
      "Getting tweets before 1183933569279787007\n",
      "1600 tweets downloaded so far\n",
      "Getting tweets before 1183889235977351169\n",
      "1700 tweets downloaded so far\n",
      "Getting tweets before 1183843230108061702\n",
      "1800 tweets downloaded so far\n",
      "Getting tweets before 1183809637457498111\n",
      "1900 tweets downloaded so far\n",
      "Getting tweets before 1183771294246035456\n",
      "2000 tweets downloaded so far\n"
     ]
    }
   ],
   "source": [
    "# pass in the search query\n",
    "new_search = \"#election2020\" + \" -filter:retweets\"\n",
    "tweets4 = get_query_tweets(new_search, 2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1991, 4) (1000, 4) (1000, 4) (2000, 4)\n"
     ]
    }
   ],
   "source": [
    "print(tweets1.shape,tweets2.shape,tweets3.shape,tweets4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets1.append(tweets2, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets.append(tweets3, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets=tweets.append(tweets4, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5991, 4)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.to_csv(r\"tweets.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all punctuation characters from each tweet and convert text to lowercase for efficient frequency counting\n",
    "# punctuation includes !\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~\n",
    "\n",
    "def remove_punctuations(item):\n",
    "    for p in punctuation:\n",
    "        item = item.strip().replace(p,'')\n",
    "    return item\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(s):\n",
    "    return [w for w in s if not w in stop_words] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet Length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'No reparations no vote! No BLACK agenda no V...</td>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Thoughts? San Francisco ::   Adam Hattersley...</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@TheJuanWilliams Why why why are you here???...</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'#BernieSanders = #Socialism which leads to #...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@AOC should re-think her endorsement of inde...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Tweet Length\n",
       "0  b'No reparations no vote! No BLACK agenda no V...            72\n",
       "1  b'Thoughts? San Francisco ::   Adam Hattersley...           153\n",
       "2  b'@TheJuanWilliams Why why why are you here???...            85\n",
       "3  b'#BernieSanders = #Socialism which leads to #...           155\n",
       "4  b'@AOC should re-think her endorsement of inde...           154"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load user comments into a dataframe\n",
    "df = pd.read_csv(\"tweets.csv\", usecols=[\"text\"])\n",
    "df.columns = [\"Tweets\"]\n",
    "\n",
    "# Remove newline from each post & drop rows with null values\n",
    "df = df.replace('\\n','', regex=True)\n",
    "df = df.dropna()\n",
    "\n",
    "df[\"Tweet Length\"]= df[\"Tweets\"].str.len() \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "tok = WordPunctTokenizer()\n",
    "pat1 = r'@[A-Za-z0-9]+'\n",
    "pat2 = r'https?://[A-Za-z0-9./]+'\n",
    "combined_pat = r'|'.join((pat1, pat2))\n",
    "def tweet_cleaner1(text):\n",
    "    \n",
    "    # HTML Decoding\n",
    "    soup = BeautifulSoup(text, 'lxml')\n",
    "    souped = soup.get_text()\n",
    "    souped = souped.replace(\"b\", \"\", 1)\n",
    "    # Remove URL links\n",
    "    stripped = re.sub(pat2, '', souped)\n",
    "    \n",
    "    # UTF-8 BOM Decoding\n",
    "    try:\n",
    "        clean = stripped.decode(\"utf-8-sig\").replace(u\"\\ufffd\", \"?\")\n",
    "    except:\n",
    "        clean = stripped\n",
    "    \n",
    "    # Remove hastags and numbers\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", clean)\n",
    "    \n",
    "    lower_case = letters_only.lower()\n",
    "    # During the letters_only process two lines above, it has created unnecessay white spaces,\n",
    "    # I will tokenize and join together to remove unneccessary white spaces\n",
    "    words = tok.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tweets = []\n",
    "\n",
    "for i in range(df.shape[0]):\n",
    "    clean_tweets.append(tweet_cleaner1(df['Tweets'][i]))\n",
    "    \n",
    "clean_tweets = pd.DataFrame(clean_tweets)\n",
    "clean_tweets.columns = [\"Tweets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweets</th>\n",
       "      <th>Tweet Length</th>\n",
       "      <th>Tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'No reparations no vote! No BLACK agenda no V...</td>\n",
       "      <td>72</td>\n",
       "      <td>[agenda, vote, ados, election, black, reparati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'Thoughts? San Francisco ::   Adam Hattersley...</td>\n",
       "      <td>153</td>\n",
       "      <td>[thoughts, rakes, adamhattersley, campaignfina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@TheJuanWilliams Why why why are you here???...</td>\n",
       "      <td>85</td>\n",
       "      <td>[election, way, thejuanwilliams, trump]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'#BernieSanders = #Socialism which leads to #...</td>\n",
       "      <td>155</td>\n",
       "      <td>[sanders, socialism, turn, xa, leads, governme...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'@AOC should re-think her endorsement of inde...</td>\n",
       "      <td>154</td>\n",
       "      <td>[endorsement, voted, republicans, ru, x, aoc, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Tweets  Tweet Length  \\\n",
       "0  b'No reparations no vote! No BLACK agenda no V...            72   \n",
       "1  b'Thoughts? San Francisco ::   Adam Hattersley...           153   \n",
       "2  b'@TheJuanWilliams Why why why are you here???...            85   \n",
       "3  b'#BernieSanders = #Socialism which leads to #...           155   \n",
       "4  b'@AOC should re-think her endorsement of inde...           154   \n",
       "\n",
       "                                              Tokens  \n",
       "0  [agenda, vote, ados, election, black, reparati...  \n",
       "1  [thoughts, rakes, adamhattersley, campaignfina...  \n",
       "2            [election, way, thejuanwilliams, trump]  \n",
       "3  [sanders, socialism, turn, xa, leads, governme...  \n",
       "4  [endorsement, voted, republicans, ru, x, aoc, ...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_tweets[\"Tweets\"] = clean_tweets['Tweets'].apply(remove_punctuations)\n",
    "\n",
    "# Download stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Replace the typing errors and combine politician names mentioned differently\n",
    "clean_tweets['Tweets'].replace({'realdonaldtrump':'trump', 'donaldtrump':'trump', 'donald':'trump', 'trumpxe':'trump', 'trumps':'trump', 'ntrump':'trump'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'elections':'election'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'presidential': 'president'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'democraticdebates':'demdebate','democraticdebate':'demdebate', 'demdebatemnthe ':'demdebate', 'cnndebate':'demdebate', 'demdebates':'demdebate','debates':'demdebate',' debate ': 'demdebate',}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'democratic ': 'democrats',  'thedemocrats ':'democrats', ' democrat ':'democrats', ' dem ':'democrats',}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'ewarren': 'warren',' elizabeth ': 'warren','elizabethwarren': 'warren','senwarren': 'warren' }, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'joebiden': 'biden',' joe ': 'biden'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({' bernie ': 'sanders','berniesanders': 'sanders', 'bernieyellsforus': 'sanders','stillsanders':'sanders'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'petebuttigieg': 'buttigieg',' pete ': 'buttigieg'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'andrewyang': 'yang',' andrew ': 'yang','yanggang': 'yang'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'kamalaharris': 'harris',' kamala ': 'harris'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'jobs':'job','factory':'job'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({' health ':'halthcare'}, inplace=True, regex=True)\n",
    "clean_tweets['Tweets'].replace({'russian':'russia'}, inplace=True, regex=True)\n",
    "\n",
    "# Tokenize the posts\n",
    "df['Tokens'] = clean_tweets['Tweets'].apply(word_tokenize).apply(set).apply(list)\n",
    "df['Tokens'] = df['Tokens'].apply(remove_stopwords)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PART B - Identifying 4 key issues mentioned in the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('x', 4391),\n",
       " ('xe', 4285),\n",
       " ('xa', 4160),\n",
       " ('election', 2759),\n",
       " ('trump', 1155),\n",
       " ('n', 987),\n",
       " ('president', 872),\n",
       " ('u', 835),\n",
       " ('demdebate', 785),\n",
       " ('rt', 691),\n",
       " ('warren', 450),\n",
       " ('biden', 330),\n",
       " ('new', 295),\n",
       " ('f', 290),\n",
       " ('china', 273),\n",
       " ('via', 264),\n",
       " ('th', 252),\n",
       " ('war', 250),\n",
       " ('trade', 250),\n",
       " ('russia', 248),\n",
       " ('xf', 246),\n",
       " ('made', 244),\n",
       " ('democrats', 235),\n",
       " ('making', 234),\n",
       " ('months', 231),\n",
       " ('washington', 229),\n",
       " ('yang', 226),\n",
       " ('progress', 224),\n",
       " ('headlines', 221),\n",
       " ('polic', 220),\n",
       " ('tonight', 207),\n",
       " ('good', 195),\n",
       " ('report', 190),\n",
       " ('win', 189),\n",
       " ('candidates', 188),\n",
       " ('vote', 184),\n",
       " ('sanders', 180),\n",
       " ('secret', 178),\n",
       " ('people', 176),\n",
       " ('one', 175),\n",
       " ('know', 169),\n",
       " ('weapon', 169),\n",
       " ('buttigieg', 168),\n",
       " ('revealed', 168),\n",
       " ('cyberwarfare', 162),\n",
       " ('like', 159),\n",
       " ('look', 158),\n",
       " ('cnn', 150),\n",
       " ('get', 148),\n",
       " ('think', 141),\n",
       " ('would', 141),\n",
       " ('candidate', 138),\n",
       " ('us', 137),\n",
       " ('need', 137),\n",
       " ('looks', 136),\n",
       " ('many', 129),\n",
       " ('maga', 127),\n",
       " ('amyklobuchar', 127),\n",
       " ('right', 126),\n",
       " ('years', 126),\n",
       " ('big', 125),\n",
       " ('bmcadory', 125),\n",
       " ('america', 121),\n",
       " ('xb', 121),\n",
       " ('got', 120),\n",
       " ('tech', 120),\n",
       " ('wrong', 119),\n",
       " ('certainly', 118),\n",
       " ('going', 112),\n",
       " ('harris', 111),\n",
       " ('watching', 109),\n",
       " ('security', 109),\n",
       " ('voters', 104),\n",
       " ('job', 104),\n",
       " ('potus', 99),\n",
       " ('companies', 97),\n",
       " ('plan', 96),\n",
       " ('take', 95),\n",
       " ('impeachment', 95),\n",
       " ('officials', 94),\n",
       " ('meeting', 94),\n",
       " ('time', 93),\n",
       " ('model', 93),\n",
       " ('want', 92),\n",
       " ('campaign', 91),\n",
       " ('support', 90),\n",
       " ('automation', 89),\n",
       " ('gop', 87),\n",
       " ('predicts', 87),\n",
       " ('make', 85),\n",
       " ('poll', 84),\n",
       " ('c', 82),\n",
       " ('news', 79),\n",
       " ('republican', 79),\n",
       " ('tulsigabbard', 78),\n",
       " ('every', 77),\n",
       " ('politics', 77),\n",
       " ('see', 75),\n",
       " ('still', 74),\n",
       " ('political', 73),\n",
       " ('icymi', 73),\n",
       " ('dems', 71),\n",
       " ('go', 70),\n",
       " ('back', 69),\n",
       " ('fourth', 69),\n",
       " ('great', 68),\n",
       " ('nyt', 68),\n",
       " ('voxdotcom', 68),\n",
       " ('first', 67),\n",
       " ('w', 67),\n",
       " ('media', 66),\n",
       " ('say', 66),\n",
       " ('state', 66),\n",
       " ('let', 65),\n",
       " ('nthe', 65),\n",
       " ('best', 65),\n",
       " ('mike', 65),\n",
       " ('says', 64),\n",
       " ('change', 61),\n",
       " ('even', 60),\n",
       " ('senate', 60),\n",
       " ('democratscandidates', 59),\n",
       " ('xba', 59),\n",
       " ('stage', 59),\n",
       " ('democratspresident', 59),\n",
       " ('way', 58),\n",
       " ('next', 58),\n",
       " ('watch', 58),\n",
       " ('destroying', 57),\n",
       " ('isaac', 56),\n",
       " ('e', 55),\n",
       " ('question', 54),\n",
       " ('help', 54),\n",
       " ('donated', 54),\n",
       " ('top', 53),\n",
       " ('ukraine', 53),\n",
       " ('davey', 53),\n",
       " ('alba', 53),\n",
       " ('policy', 52),\n",
       " ('please', 51),\n",
       " ('lifelong', 51),\n",
       " ('demdemdebatei', 51),\n",
       " ('love', 50),\n",
       " ('country', 50),\n",
       " ('trying', 50),\n",
       " ('said', 50),\n",
       " ('american', 50),\n",
       " ('mikemonroerm', 50),\n",
       " ('christianllamar', 50),\n",
       " ('another', 49),\n",
       " ('running', 49),\n",
       " ('foreign', 49),\n",
       " ('year', 48),\n",
       " ('ohio', 48),\n",
       " ('something', 48),\n",
       " ('tulsi', 48),\n",
       " ('cortez', 48),\n",
       " ('alexandria', 48),\n",
       " ('ocasio', 48),\n",
       " ('today', 48),\n",
       " ('vs', 48),\n",
       " ('choice', 47),\n",
       " ('really', 46),\n",
       " ('done', 46),\n",
       " ('states', 45),\n",
       " ('run', 45),\n",
       " ('xc', 45),\n",
       " ('keep', 45),\n",
       " ('voting', 45),\n",
       " ('getting', 45),\n",
       " ('race', 45),\n",
       " ('day', 44),\n",
       " ('b', 44),\n",
       " ('americans', 44),\n",
       " ('read', 44),\n",
       " ('nytimes', 44),\n",
       " ('forbes', 44),\n",
       " ('republicans', 43),\n",
       " ('important', 43),\n",
       " ('lost', 43),\n",
       " ('live', 43),\n",
       " ('open', 43),\n",
       " ('oh', 42),\n",
       " ('work', 42),\n",
       " ('talking', 42),\n",
       " ('check', 42),\n",
       " ('house', 42),\n",
       " ('r', 42),\n",
       " ('thing', 41),\n",
       " ('sure', 41),\n",
       " ('last', 41),\n",
       " ('anyone', 41),\n",
       " ('money', 41),\n",
       " ('onstage', 41),\n",
       " ('trum', 41),\n",
       " ('pay', 41),\n",
       " ('facebook', 41),\n",
       " ('memo', 41),\n",
       " ('congresswoman', 41),\n",
       " ('aoc', 40),\n",
       " ('come', 40),\n",
       " ('never', 40),\n",
       " ('must', 40),\n",
       " ('numbers', 40),\n",
       " ('find', 40),\n",
       " ('attention', 40),\n",
       " ('added', 40),\n",
       " ('party', 39),\n",
       " ('everyone', 39),\n",
       " ('left', 39),\n",
       " ('far', 39),\n",
       " ('healthcare', 39),\n",
       " ('amandionair', 39),\n",
       " ('referendum', 39),\n",
       " ('government', 38),\n",
       " ('corybooker', 38),\n",
       " ('talk', 38),\n",
       " ('dnc', 38),\n",
       " ('ban', 38),\n",
       " ('foxnews', 38),\n",
       " ('much', 37),\n",
       " ('power', 37),\n",
       " ('white', 37),\n",
       " ('well', 37),\n",
       " ('women', 37),\n",
       " ('electi', 37),\n",
       " ('tax', 37),\n",
       " ('asdemocratsto', 37),\n",
       " ('backsanderssanders', 37),\n",
       " ('real', 36),\n",
       " ('ask', 36),\n",
       " ('things', 36),\n",
       " ('join', 36),\n",
       " ('life', 36),\n",
       " ('attacks', 36),\n",
       " ('syria', 35),\n",
       " ('could', 35),\n",
       " ('gabbard', 35),\n",
       " ('betoorourke', 35),\n",
       " ('story', 35),\n",
       " ('climate', 35),\n",
       " ('nothing', 35),\n",
       " ('comes', 34),\n",
       " ('point', 34),\n",
       " ('believe', 34),\n",
       " ('video', 34),\n",
       " ('worth', 34),\n",
       " ('beto', 34),\n",
       " ('polls', 34),\n",
       " ('xef', 34),\n",
       " ('bbcworld', 34),\n",
       " ('kag', 33),\n",
       " ('night', 33),\n",
       " ('obama', 33),\n",
       " ('voter', 33),\n",
       " ('hunter', 33),\n",
       " ('show', 33),\n",
       " ('xbb', 32),\n",
       " ('needs', 32),\n",
       " ('asked', 32),\n",
       " ('impeach', 32),\n",
       " ('hope', 32),\n",
       " ('booker', 32),\n",
       " ('around', 32),\n",
       " ('medicare', 32),\n",
       " ('two', 31),\n",
       " ('put', 31),\n",
       " ('issues', 31),\n",
       " ('stop', 31),\n",
       " ('already', 31),\n",
       " ('na', 31),\n",
       " ('better', 31),\n",
       " ('primary', 31),\n",
       " ('democraticdemdebatefor', 31),\n",
       " ('shelbyfleig', 31),\n",
       " ('black', 30),\n",
       " ('chance', 30),\n",
       " ('twitter', 30),\n",
       " ('thank', 30),\n",
       " ('also', 30),\n",
       " ('analysis', 30),\n",
       " ('days', 30),\n",
       " ('pm', 30),\n",
       " ('interference', 30),\n",
       " ('week', 30),\n",
       " ('wait', 29),\n",
       " ('elected', 29),\n",
       " ('woman', 29),\n",
       " ('college', 29),\n",
       " ('end', 29),\n",
       " ('social', 29),\n",
       " ('full', 29),\n",
       " ('united', 28),\n",
       " ('part', 28),\n",
       " ('insurance', 28),\n",
       " ('yet', 28),\n",
       " ('promises', 28),\n",
       " ('p', 28),\n",
       " ('control', 28),\n",
       " ('city', 28),\n",
       " ('yes', 27),\n",
       " ('damn', 27),\n",
       " ('times', 27),\n",
       " ('tuesday', 27),\n",
       " ('thinking', 27),\n",
       " ('ba', 27),\n",
       " ('agree', 27),\n",
       " ('may', 27),\n",
       " ('looking', 27),\n",
       " ('saying', 27),\n",
       " ('waiting', 27),\n",
       " ('repadamschiff', 27),\n",
       " ('hear', 26),\n",
       " ('l', 26),\n",
       " ('anything', 26),\n",
       " ('favorite', 26),\n",
       " ('bad', 26),\n",
       " ('wants', 26),\n",
       " ('use', 26),\n",
       " ('tomorrow', 26),\n",
       " ('long', 26),\n",
       " ('answer', 26),\n",
       " ('goaxim', 26),\n",
       " ('xaf', 25),\n",
       " ('york', 25),\n",
       " ('nmy', 25),\n",
       " ('votes', 25),\n",
       " ('since', 25),\n",
       " ('early', 25),\n",
       " ('office', 25),\n",
       " ('ahead', 25),\n",
       " ('instead', 25),\n",
       " ('gun', 25),\n",
       " ('actually', 25),\n",
       " ('speakerpelosi', 25),\n",
       " ('nyou', 25),\n",
       " ('paid', 25),\n",
       " ('starts', 25),\n",
       " ('held', 24),\n",
       " ('hey', 24),\n",
       " ('fight', 24),\n",
       " ('stand', 24),\n",
       " ('someone', 24),\n",
       " ('winning', 24),\n",
       " ('away', 24),\n",
       " ('man', 24),\n",
       " ('used', 24),\n",
       " ('rights', 24),\n",
       " ('fund', 24),\n",
       " ('calling', 24),\n",
       " ('coming', 24),\n",
       " ('attack', 24),\n",
       " ('tweet', 24),\n",
       " ('influence', 24),\n",
       " ('empty', 24),\n",
       " ('thelead', 24),\n",
       " ('three', 23),\n",
       " ('democraticdemdebatewas', 23),\n",
       " ('iowa', 23),\n",
       " ('hard', 23),\n",
       " ('bernie', 23),\n",
       " ('old', 23),\n",
       " ('fact', 23),\n",
       " ('klobuchar', 23),\n",
       " ('true', 23),\n",
       " ('wow', 23),\n",
       " ('call', 23),\n",
       " ('ready', 23),\n",
       " ('nsandersn', 23),\n",
       " ('corporate', 23),\n",
       " ('billion', 23),\n",
       " ('investigation', 23),\n",
       " ('abuse', 23),\n",
       " ('legislatures', 23),\n",
       " ('million', 22),\n",
       " ('xbd', 22),\n",
       " ('front', 22),\n",
       " ('ever', 22),\n",
       " ('usa', 22),\n",
       " ('november', 22),\n",
       " ('abc', 22),\n",
       " ('rep', 22),\n",
       " ('lose', 22),\n",
       " ('free', 22),\n",
       " ('national', 22),\n",
       " ('treatment', 22),\n",
       " ('pr', 22),\n",
       " ('world', 22),\n",
       " ('ad', 22),\n",
       " ('lindseygrahamsc', 22),\n",
       " ('key', 22),\n",
       " ('carolina', 22),\n",
       " ('tag', 22),\n",
       " ('lead', 22),\n",
       " ('foxandfriends', 22),\n",
       " ('dozens', 22),\n",
       " ('councils', 22),\n",
       " ('updates', 22),\n",
       " ('xaa', 21),\n",
       " ('idea', 21),\n",
       " ('feel', 21),\n",
       " ('democracy', 21),\n",
       " ('everything', 21),\n",
       " ('questions', 21),\n",
       " ('place', 21),\n",
       " ('beat', 21),\n",
       " ('october', 21),\n",
       " ('hillary', 21),\n",
       " ('sen', 21),\n",
       " ('corruption', 21),\n",
       " ('name', 21),\n",
       " ('models', 21),\n",
       " ('taking', 21),\n",
       " ('gets', 21),\n",
       " ('hillaryclinton', 21),\n",
       " ('google', 21),\n",
       " ('shows', 21),\n",
       " ('et', 21),\n",
       " ('giving', 21),\n",
       " ('post', 21),\n",
       " ('runforsomething', 21),\n",
       " ('mental', 21),\n",
       " ('substance', 21),\n",
       " ('price', 21),\n",
       " ('illness', 21),\n",
       " ('bringingreceipts', 21),\n",
       " ('public', 20),\n",
       " ('biggest', 20),\n",
       " ('face', 20),\n",
       " ('gon', 20),\n",
       " ('ni', 20),\n",
       " ('single', 20),\n",
       " ('turnout', 20),\n",
       " ('presidency', 20),\n",
       " ('h', 20),\n",
       " ('lot', 20),\n",
       " ('purchase', 20),\n",
       " ('case', 20),\n",
       " ('nthis', 20),\n",
       " ('shirt', 20),\n",
       " ('lookin', 20),\n",
       " ('impeaching', 19),\n",
       " ('give', 19),\n",
       " ('person', 19),\n",
       " ('tell', 19),\n",
       " ('start', 19),\n",
       " ('moody', 19),\n",
       " ('clear', 19),\n",
       " ('game', 19),\n",
       " ('juliancastro', 19),\n",
       " ('seriously', 19),\n",
       " ('latest', 19),\n",
       " ('meet', 19),\n",
       " ('maybe', 19),\n",
       " ('general', 19),\n",
       " ('criticized', 19),\n",
       " ('thomaskaine', 19),\n",
       " ('clue', 19),\n",
       " ('presidentdemdebatelive', 19),\n",
       " ('product', 19),\n",
       " ('discounts', 19),\n",
       " ('nsale', 19),\n",
       " ('agilenthawking', 19),\n",
       " ('pric', 19),\n",
       " ('ticket', 18),\n",
       " ('sensanders', 18),\n",
       " ('truth', 18),\n",
       " ('remember', 18),\n",
       " ('nomination', 18),\n",
       " ('deal', 18),\n",
       " ('issue', 18),\n",
       " ('across', 18),\n",
       " ('breaking', 18),\n",
       " ('seems', 18),\n",
       " ('reason', 18),\n",
       " ('thought', 18),\n",
       " ('pre', 18),\n",
       " ('enough', 18),\n",
       " ('k', 18),\n",
       " ('might', 18),\n",
       " ('supporters', 18),\n",
       " ('debate', 18),\n",
       " ('listen', 18),\n",
       " ('follow', 18),\n",
       " ('cbsnews', 18),\n",
       " ('bloomberg', 18),\n",
       " ('msnbc', 18),\n",
       " ('north', 18),\n",
       " ('care', 17),\n",
       " ('history', 17),\n",
       " ('matter', 17),\n",
       " ('happen', 17),\n",
       " ('vice', 17),\n",
       " ('administration', 17),\n",
       " ('took', 17),\n",
       " ('exactly', 17),\n",
       " ('continue', 17),\n",
       " ('exposecnn', 17),\n",
       " ('able', 17),\n",
       " ('tomsteyer', 17)]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = []\n",
    "# Count the frequency of the words\n",
    "for i in range(len(df[\"Tokens\"])):\n",
    "    count += df.iloc[i]['Tokens']\n",
    "count = [x.lower() for x in count]\n",
    "word_freq = nltk.FreqDist(count)\n",
    "\n",
    "# 500 most frequent words\n",
    "top_words = word_freq.most_common(500)\n",
    "top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis\n",
    "Top Republican candidate - Donald Trump <br>\n",
    "Top Democratic candidate - Elizabeth Warren <br><br>\n",
    "Top 6 issues found:\n",
    "1. China\n",
    "2. Trade war\n",
    "3. Russia\n",
    "4. Cyberwarfare\n",
    "5. MAGA\n",
    "6. Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
